---
title: "Analysing TED talk content"
author: "HASS Taskforce"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(readr)
library(dplyr)
library(tidytext)
library(ggplot2)
library(stringr)
library(tidyr)
```

# Final Exercise: Analyzing TED talks

In this exercise, you'll apply text analysis techniques to the data from the [TED talks as Data](https://culturalanalytics.org/article/11044-ted-talks-as-data) paper, published in the Journal of Cultural Analytics. This dataset contains transcripts of TED talks, where each row is a talk, with an ID, headline, text, and speaker field.

Journal of Cultural Analytics
Kinnaird, Katherine M., and John Laudun. 2019. “TED Talks as Data.” Journal of Cultural Analytics 4 (2). [https://doi.org/10.22148/16.042].

The data can be found following the link in the citation below:

Kinnaird, Katherine M. and John Laudun. 2018. TED Talks Data Set. (https://github.com/johnlaudun/tedtalks/tree/master/data/Release_v0).

## Download the data.

```{r}
url <- "https://raw.githubusercontent.com/kinnaird-laudun/data/refs/heads/main/Release_v0/TEDonly_final.csv"

ted_talks <- read_csv(url, col_select = c("Talk_ID", "headline", "text", "speaker_1")) |> 
  rename(talk_id = Talk_ID, speaker = speaker_1) |> 
  mutate(text = str_remove_all(text, "\\([^\\)]+\\)"))
```

## 1. Tokenize the tweets into individual lowercase words, and remove stop words

```{r}
tidy_ted <- ted_talks |>
  unnest_tokens(word, text, to_lower = TRUE) |>
  anti_join(stop_words)
```

## 2. Visualize the 20 most frequently used words across all talks.

```{r}
tidy_ted |>
  count(word, sort = TRUE) |>
  head(20) |>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(n, word)) +
  geom_col() +
  labs(x = "Frequency", y = NULL) +
  theme_minimal()
```

3. Use the Bing lexicon to score overall sentiment by talk.

```{r}
bing <- get_sentiments("bing")

ted_sentiment <- tidy_ted |>
  inner_join(bing) |>
  count(talk_id, headline, speaker, sentiment) |>
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |>
  mutate(sentiment_score = positive - negative)
```

4. Plot the sentiment scores in a histogram

```{r}
ted_sentiment |> 
  ggplot(aes(sentiment_score)) +
  geom_histogram() +
  labs(title = "Sentiment scores across TED Talks",
       x = "Net sentiment score", y = "Number of talks") +
  theme_minimal()
```

5. Use the Bing lexicon to look at the most common positive and negative words

```{r}
bing_word_counts <- tidy_ted |>
  inner_join(bing) |>
  count(word, sentiment, sort = TRUE) |> 
  ungroup()

bing_word_counts
```

6. Visualise the top ten words contributing to positive and negative sentiment

```{r}
bing_word_counts |> 
  group_by(sentiment) |> 
  slice_max(n, n = 10) |> 
  ungroup() |> 
  mutate(word = reorder(word, n)) |> 
  ggplot(aes(x = n, y = word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL) +
  theme_minimal()
```

7. Explore frequent word pairs in TED talks (after removing stop words)

```{r}
ted_bigrams <- ted_talks |> 
  select(talk_id, text) |> 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) |> 
  separate(bigram, c("word1", "word2"), sep = " ") |> 
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) |> 
  count(word1, word2, sort = TRUE)

head(ted_bigrams, 20)
```
