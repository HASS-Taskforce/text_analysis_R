---
title: "Text analysis with R"
author: "HASS Taskforce"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Notes about this workshop

All the material in this workshop is adapted from the [Text Mining with R](https://www.tidytextmining.com/) book, by [Julia Silge](https://juliasilge.com/) and [David Robinson](http://varianceexplained.org/); which is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 3.0 United States License](https://creativecommons.org/licenses/by-nc-sa/3.0/us/). This book uses the [tidytext](https://juliasilge.github.io/tidytext/) R package, you may find this [documentation](https://www.quantargo.com/help/r/latest/packages/tidytext/0.3.1) on the specific functions available in the package to be useful.

This is an introduction to the tidy text mining framework along with a collection of examples, but it is not a complete exploration of natural language processing. The [CRAN Task View on Natural Language Processing](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html) provides details on other ways to use R for computational linguistics. There are several areas that you may want to explore in more detail according to your needs.

* **Clustering, classification, and prediction**: Machine learning on text is a vast topic that could easily fill its own volume. We introduce here one method of unsupervised clustering (topic modeling), but many more machine learning algorithms can be used in dealing with text.

* **Word embedding**: One popular modern approach for text analysis is to map words to vector representations, which can then be used to examine linguistic relationships between words and to classify text. Such representations of words are not tidy in the sense that we consider here, but have found powerful applications in machine learning algorithms.

* **More complex tokenization**: The tidytext package trusts the [tokenizers package](https://CRAN.R-project.org/package=tokenizers) (Mullen 2016) to perform tokenization, which itself wraps a variety of tokenizers with a consistent interface, but many others exist for specific applications.

* **Languages other than English**: Some users have had success applying tidytext to their text mining needs for languages other than English, but we don’t cover any such examples here.

We will use many third-party packages in this workshop. Prior to the session, please run:

`install.packages(c("tidyverse", "knitr", "tidytext", "janeaustenr", "topicmodels", "gutenbergr"))`

## The tidy text format - preprocessing

Tidy data has a specific structure:

* Each variable is a column
* Each observation is a row
* Each type of observational unit is a table

The tidy text format is a table with one-token-per-row. 

A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. 

For tidy text mining, the token that is stored in each row is most often a single word, but can also be an n-gram, sentence, or paragraph.

Check out this cool app for a [demo of tidy text](https://tidy-shakespeare.herokuapp.com/).

### Un-nesting tokens

Emily Dickinson wrote some lovely text in her time.

```{r}
text <- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")

text
```

In order to turn it into a tidy text dataset, we first need to put it into a data frame.

A tibble is a modern class of data frame within R, available in the dplyr and tibble packages, that has a convenient print method, will not convert strings to factors, and does not use row names. Tibbles are great for use with tidy tools.

```{r}

```

This data frame containing text isn’t yet compatible with tidy text analysis: each row is made up of multiple combined words. We need to convert this so that it has one-token-per-document-per-row.

```{r}

```

* Other columns, such as the line number each word came from, are retained.
* Punctuation has been stripped.
* By default, `unnest_tokens()` converts the tokens to lowercase, which makes them easier to compare or combine with other datasets. (Use the `to_lower = FALSE` argument to turn off this behavior).

### Worked example: tidying the works of Jane Austen

The `janeaustenr` package provides the text of Jane Austen’s completed, published novels in a one-row-per-line format, where a line is analogous to a literal printed line in a physical book.

We use `mutate()` to annotate a linenumber quantity to keep track of lines in the original format and a chapter (using a regex) to find where all the chapters are.

```{r}

```

To work with this as a tidy dataset, we need to restructure it in the one-token-per-row format.

```{r}

```

Now that the data is in one-word-per-row format, we can manipulate it with tidy tools like `dplyr`. Often in text analysis, we will want to remove stop words; stop words are words that are not useful for an analysis, typically extremely common words such as “the”, “of”, “to”, and so forth in English. We can remove stop words with an `anti_join()`.

```{r}

```

## Word frequencies

We can use dplyr’s `count()` to find the most common words in all the books as a whole.

```{r}

```

Because we’ve been using tidy tools, our word counts are stored in a tidy data frame. This allows us to pipe this directly to the `ggplot2` package, for example to create a visualization of the most common words.

```{r}

```

## Understanding tf-idf (Term Frequency-Inverse Document Frequency)

tf-idf is a numerical statistic used in text analysis to reflect how important a word is to a document within a collection of documents (or corpus). It combines two key concepts:

1. **Term Frequency (tf)**: How often a word appears in a document.
2. **Inverse Document Frequency (idf)**: How rare or common a word is across all documents in the corpus.

Imagine you're an archivist sorting through a large collection of books.

Term Frequency (tf) is like counting how many times a specific word appears in a single book. If "dragon" appears 10 times in a fantasy novel, it has a high term frequency for that book.

Inverse Document Frequency (idf) is like checking how many books in your entire library contain that word. If "dragon" appears in many books, its IDF would be low, as it's not very unique. If "Hogwarts" only appears in a few books, its IDF would be high, as it's more distinctive.

tf-idf multiplies these two factors together. Words with high tf-idf scores appear frequently in a specific document but are rare across the entire collection.

### Why is tf-idf useful?

It helps identify words that are characteristic or important for a particular document within a larger collection. Common words like "the" or "and" will have low tf-idf scores because they appear in almost every document. Unique or specialized terms will have higher tf-idf scores, helping to distinguish documents from each other.

To learn more, check out the [tf-idf chapter](https://www.tidytextmining.com/tfidf) in the tidytext book.

## Sentiment analysis with tidy data

When we read text, we use our understanding of the emotional intent of words to infer whether a section of text is positive or negative, or perhaps characterized by some other more nuanced emotion like surprise or disgust.

One way to analyze the sentiment of a text is to consider the text as a combination of its individual words and the sentiment content of the whole text as the sum of the sentiment content of the individual words. This isn’t the only way to approach sentiment analysis, but it is an often-used approach, and an approach that naturally takes advantage of the tidy tool ecosystem.

### Sentiment datasets

The `tidytext` package provides access to several sentiment lexicons. Three general-purpose lexicons are:

* AFINN from [Finn Årup Nielsen](https://www2.imm.dtu.dk/pubdb/pubs/6010-full.html),
* bing from [Bing Liu and collaborators](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), and
* nrc from [Saif Mohammad and Peter Turney](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm).

All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. 

We will use the `bing` lexicon, which categorizes words in a binary fashion into positive and negative categories.

```{r}

```

How were these sentiment lexicons put together and validated? They were constructed via either crowdsourcing (using, for example, Amazon Mechanical Turk) or by the labor of one of the authors, and were validated using some combination of crowdsourcing again, restaurant or movie reviews, or Twitter data.

There are also some domain-specific sentiment lexicons available, constructed to be used with text from a specific content area. [Section 5.3.1](https://www.tidytextmining.com/dtm#financial) from the tidytext book explores an analysis using a sentiment lexicon specifically for finance.

Not every English word is in the lexicons because many English words are neutral. These methods do not take into account qualifiers before a word, such as in “no good” or “not true”; a lexicon-based method like this is based on unigrams only.

One last caveat is that the size of the chunk of text that we use to add up unigram sentiment scores can have an effect on an analysis. A text the size of many paragraphs can often have positive and negative sentiment averaged out to about zero, while sentence-sized or paragraph-sized text often works better.

### Sentiment analysis with inner join

What are the most common positive words in Emma?

```{r}

```


Or instead we could examine how sentiment changes during each novel. Let’s find a sentiment score for each word using the same lexicon, then count the number of positive and negative words in defined sections of each novel.

```{r}

```

Now we can plot these sentiment scores across the plot trajectory of each novel.

```{r}

```

### Most common positive and negative words

We can analyze word counts that contribute to each sentiment in Jane Austen's books.

```{r}

```

We can also look at this visually

```{r}

```

Notice that the word “miss” is coded as negative, but in Jane Austen's works it is used as a title for young, unmarried women. We may want to consider adding “miss” to a custom stop-words list.

## N-grams (bigrams)

### Tokenizing by n-gram

We’ve been using the `unnest_tokens` function to tokenize by word, or sometimes by sentence, which is useful for the kinds of sentiment and frequency analyses we’ve been doing so far. But we can also use the function to tokenize into consecutive sequences of words, called n-grams. By seeing how often word X is followed by word Y, we can then build a model of the relationships between them.

We do this by adding the `token = "ngrams"` option to `unnest_tokens()`, and setting n to the number of words we wish to capture in each n-gram. When we set n to 2, we are examining pairs of two consecutive words, often called “bigrams”:

```{r}

```

### Counting and filtering n-grams

Our usual tidy tools apply equally well to n-gram analysis. We can examine the most common bigrams using dplyr’s `count()`:

```{r}

```

As one might expect, a lot of the most common bigrams are pairs of common (uninteresting) words, such as *of the* and *to be*: what we call “stop-words”. This is a useful time to use tidyr’s `separate()`, which splits a column into multiple based on a delimiter. This lets us separate it into two columns, “word1” and “word2”, at which point we can remove cases where either is a stop-word.

```{r}

```

We can see that names (whether first and last or with a salutation) are the most common pairs in Jane Austen books.

### Analyzing bigrams

This one-bigram-per-row format is helpful for exploratory analyses of the text. As a simple example, we might be interested in the most common “streets” mentioned in each book:

```{r}

```

A lot more can be done with n-grams, this is just a taste. See [Chapter 4 of Text Mining with R](https://www.tidytextmining.com/ngrams) for more :)

## Topic modeling

In text mining, we often have collections of documents, such as blog posts or news articles, that we’d like to divide into natural groups so that we can understand them separately. Topic modeling is a method for unsupervised classification of such documents, similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for.

Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language.

### Latent Dirichlet allocation

Latent Dirichlet allocation is one of the most common algorithms for topic modeling. Without diving into the math behind the model, we can understand it as being guided by two principles.

**Every document is a mixture of topics**. We imagine that each document may contain words from several topics in particular proportions. For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”

**Every topic is a mixture of words**. For example, we could imagine a two-topic model of American news, with one topic for “politics” and one for “entertainment.” The most common words in the politics topic might be “President”, “Congress”, and “government”, while the entertainment topic may be made up of words such as “movies”, “television”, and “actor”. Importantly, words can be shared between topics; a word like “budget” might appear in both equally.

LDA is a mathematical method for estimating both of these at the same time: finding the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document.

We will use the `topicmodels` package, which requires the text to be structured as a *Document Term Matrix* (or DTM). This is a matrix where:

* each row represents one document (such as a book or article),
* each column represents one term, and
* each value (typically) contains the number of appearances of that term in that document.

DTM objects cannot be used directly with tidy tools, just as tidy data frames cannot be used as input for most text mining packages. Thus, the tidytext package provides two verbs that convert between the two formats:

* `tidy()` turns a document-term matrix into a tidy data frame. This verb comes from the `broom` package, which provides similar tidying functions for many statistical models and objects.

* `cast_dtm()` turns a tidy one-term-per-row data frame into a DocumentTermMatrix object.

## Worked example: the great library heist

When examining a statistical method, it can be useful to try it on a simple case where you know the “right answer”. For example, we could collect a set of documents that definitely relate to four separate topics, then perform topic modeling to see whether the algorithm can correctly distinguish the four groups. This lets us double-check that the method is useful, and gain a sense of how and when it can go wrong. We’ll try this with some data from classic literature.

Suppose a vandal has broken into your study and torn apart four of your books:

* Great Expectations by Charles Dickens
* The War of the Worlds by H.G. Wells
* Twenty Thousand Leagues Under the Sea by Jules Verne
* Pride and Prejudice by Jane Austen

This vandal has torn the books into individual chapters, and left them in one large pile. How can we restore these disorganized chapters to their original books? This is a challenging problem since the individual chapters are **unlabeled**: we don’t know what words might distinguish them into groups. We’ll thus use topic modeling to discover how chapters cluster into distinct topics, each of them (presumably) representing one of the books.

We’ll retrieve the text of these four books using the `gutenbergr` package. [Project Gutenberg](https://www.gutenberg.org/) is a free online library that offers over 60,000 eBooks, mostly classic literature. It makes these texts freely available to the public in digital formats like plain text, which is very useful for NLP.

```{r}

```

As pre-processing, we: 

* divide these into chapters, 
* use tidytext’s `unnest_tokens()` to separate them into words, 
* use `str_extract()` because texts from Project Gutenberg have some examples of words with underscores around them to indicate emphasis (like italics). The tokenizer treat these as words, but we don’t want to count “_any_” separately from “any” 
* then remove stop_words. 

We’re treating every chapter as a separate “document”, each with a name like *Great Expectations_1* or *Pride and Prejudice_11*. (In other applications, each document might be one newspaper article, or one blog post).

```{r}

```

### LDA on chapters

Right now our data frame `word_counts` is in a tidy form, with one-term-per-document-per-row, but the `topicmodels` package requires a `DocumentTermMatrix`. This means we need one row for each document (in this case, book chapter), one column for each unique word, and the values in each cell should be the number of times that word appears in that document.

```{r}

```

We can then use the `LDA()` function to create a four-topic model. In this case we know we’re looking for four topics because there are four books; in other problems we may need to try a few different values of k. We set a seed so that the output of the model is predictable/reproducible.

```{r}

```

We can use the aforementioned `tidy()` function to extract the per-topic-per-word probabilities, called β (“beta”), from the LDA model.

```{r}

```

For each combination of topic and term, the model computes the probability of that term being generated from that topic.

We could use dplyr’s `slice_max()` to find the top 5 terms within each topic.

```{r}

```

We can visualise this with ggplot:

```{r}

```

These topics are pretty clearly associated with the four books! There’s no question that the topic of “captain”, “nautilus”, “sea”, and “nemo” belongs to *Twenty Thousand Leagues Under the Sea*, and that “jane”, “darcy”, and “elizabeth” belongs to *Pride and Prejudice*. We see “pip” and “joe” from *Great Expectations* and “alice”, “queen”, and “king” from *Alice's Adventures in Wonderland*. We also notice that, in line with LDA being a “fuzzy clustering” method, there can be words in common between multiple topics, such as “time” in topics 2 and 3.

The next step would be to put these chapters back in the correct books, check out [Chapter 6 of Text Mining with R](https://www.tidytextmining.com/topicmodeling#per-document) to see how this is done.

## Resources and to learn more

* [Text Mining with R](https://www.tidytextmining.com/) book, by [Julia Silge](https://juliasilge.com/) and [David Robinson](http://varianceexplained.org/)

* [Text Mining with Tidy Data Principles, Julia Silge](https://juliasilge.github.io/tidytext-tutorial/)